{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyceful-lee/AI-Tile-Placement/blob/master/project4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htS6svoNSJ56"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "teamId = 1325\n",
        "worldId = 0\n",
        "url = \"https://www.notexponential.com/aip2pgaming/api/rl/gw.php\"\n",
        "\n",
        "\n",
        "payload={}\n",
        "headers = {\n",
        "  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
        "  'x-api-key': 'b65d58eeb9360bebb41e',\n",
        "  'userid': '1096',\n",
        "  'Content-Type': 'application/x-www-form-urlencoded'}\n",
        "\n",
        "environment_rows = 41\n",
        "environment_columns = 41\n",
        "\n",
        "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
        "actions = ['N', 'S', 'E', 'W']\n",
        "current_reward = -0.1\n",
        "newState= None\n",
        "x = 0\n",
        "y = 0\n",
        "\n",
        "#teamid 1330\n",
        "def create_team(name):\n",
        "  url = \"https://www.notexponential.com/aip2pgaming/api/index.php\"\n",
        "  payload = {'type': 'team',\n",
        "        'name': f'{name}',\n",
        "        }\n",
        "  files = [\n",
        "\n",
        "    ]\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
        "  print(response.text)\n",
        "\n",
        "#create_team('prj4')\n",
        "\n",
        "def add_meb(teamId):\n",
        "  url = \"https://www.notexponential.com/aip2pgaming/api/index.php\"\n",
        "  payload = {'type': 'member',\n",
        "        'teamId': f'{teamId}',\n",
        "        'userid': '1109'\n",
        "        }\n",
        "  files = [\n",
        "\n",
        "    ]\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
        "  print(response.text)\n",
        "#add_meb(1331)\n",
        "def get_runs(teamId, count):\n",
        "  query_url = \"https://www.notexponential.com/aip2pgaming/api/rl/score.php?type=runs&teamId={teamId}&count={count}\"\n",
        "  response = requests.request(\"GET\", query_url, headers=headers, data=payload)\n",
        "  data = json.loads(response.text)\n",
        "  print(data)\n",
        "#get_runs(1330, 0)\n",
        "\n",
        "def getMyTeam():\n",
        "    query_url = \"https://www.notexponential.com/aip2pgaming/api/index.php?type=myTeams\"\n",
        "    response = requests.request(\"GET\", query_url, headers=headers, data=payload)\n",
        "    team_data = json.loads(response.text)\n",
        "    team = team_data[\"myTeams\"]\n",
        "    print(f\"Teams:{team}\")\n",
        "    return team\n",
        "#getMyTeam()\n",
        "\n",
        "def reset_team(teamId):\n",
        "  query_url = f\"https://www.notexponential.com/aip2pgaming/api/rl/reset.php?teamId={teamId}&otp=5712768807\"\n",
        "  response = requests.request(\"GET\", query_url, headers=headers, data=payload)\n",
        "  print(response.text)\n",
        "#reset_team(1308)\n",
        "\n",
        "def enter_world(teamId, worldId):\n",
        "  payload = {'type': 'enter',\n",
        "        'teamId': f'{teamId}',\n",
        "        'worldId': f'{worldId}'}\n",
        "  files = [\n",
        "\n",
        "    ]\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
        "  data = json.loads(response.text)\n",
        "  print(data)\n",
        "#enter_world(1308, 0)\n",
        "\n",
        "def get_location(teamId):\n",
        "  query_url = \"https://www.notexponential.com/aip2pgaming/api/rl/gw.php?type=location&teamId={teamId}\"\n",
        "  response = requests.request(\"GET\", query_url, headers=headers, data=payload)\n",
        "  print(response.text)\n",
        "#get_location(1308)\n",
        "\n",
        "def get_state(teamId, move, worldId, x, y):\n",
        "  global current_reward\n",
        "\n",
        "  print(move)\n",
        "  payload = {'type': 'move',\n",
        "        'teamId': f'{teamId}',\n",
        "          'move': move,\n",
        "        'worldId': f'{worldId}'}\n",
        "  files = [\n",
        "\n",
        "    ]\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
        "  data = json.loads(response.text)\n",
        "  print(data)\n",
        "  reward = data['reward']\n",
        "  position = data['newState']\n",
        "  print(reward)\n",
        "  current_reward = reward\n",
        "  current_pos = position\n",
        "  print(position)\n",
        "  if(position!= None):\n",
        "    x=int(position['x'])\n",
        "    y=int(position['y'])\n",
        "    print(x)\n",
        "    print(y)\n",
        "  else:\n",
        "    if(move==\"N\"):\n",
        "      y +=1\n",
        "    if(move==\"S\"):\n",
        "      y -=1\n",
        "    if(move==\"E\"):\n",
        "      x +=1\n",
        "    if(move==\"W\"):\n",
        "      x -=1\n",
        "  return x,y,reward\n",
        "\n",
        "#get_runs(1309, 1)\n",
        "#enter_world(1308,0)\n",
        "#get_location(1307)\n",
        "#enter_world(1309, 1)\n",
        "#get_state(1308, \"N\", 0,0,39)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Win state reward is 10000.\n",
        "\n",
        "Lose state reward is -10000.\n",
        "\n",
        "The normal reward is -0.1\n",
        "\n",
        "\n",
        "Reward:10000\n",
        "\n",
        "{'code': 'OK', 'worldId': 0, 'runId': '6237', 'reward': 10000, 'scoreIncrement': 0, 'newState': None}\n",
        "\n",
        "\n",
        "{'x': 0, 'y': '39'} +N\n",
        "{'x': 1, 'y': '19'}\n",
        "{'x': 39, 'y': '39'}+N\n",
        "{'x': 19, 'y': '19'}+N\n",
        "\n",
        "Reward:-10000\n",
        "\n",
        "{'code': 'OK', 'worldId': 0, 'runId': '6537', 'reward': -10000, 'scoreIncrement': -1094.19, 'newState': None}\n",
        "\n",
        "(1, 20, -10000)\n",
        "(0,19,-10000)\n",
        "(19,1,-10000)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bb2zINOXSk-7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnCfO5tVG0LJ"
      },
      "source": [
        "#check whether the state is a terminal state with reward feedback\n",
        "def is_terminal(current_reward):\n",
        "  #if the reward for this location is -0.1, then it is not a terminal state\n",
        "  if current_reward == -0.1:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "#Select the move action\n",
        "def get_action(x,y):\n",
        "  return np.argmax(q_values[x, y])\n",
        "\n",
        "\n",
        "#get the position and reward info with the determined action\n",
        "#return x,y,reward\n",
        "def get_pos_and_reward(action,x,y):\n",
        "  if actions[action]==\"N\":\n",
        "    return get_state(teamId,\"N\",worldId,x,y)\n",
        "  if actions[action]==\"S\":\n",
        "    return get_state(teamId,\"S\",worldId,x,y)\n",
        "  if actions[action]==\"E\":\n",
        "    return get_state(teamId,\"E\",worldId,x,y)\n",
        "  if actions[action]==\"W\":\n",
        "    return get_state(teamId,\"W\",worldId,x,y)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N5BB0m0JHIn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c2fd5718-eae2-4df5-d84f-0f6d998aa45a"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "def update_grid(data, good_term_states, bad_term_states, obstacles, run_num, epoch, world, location):\n",
        "    pyplot.figure(1)\n",
        "    pyplot.clf()\n",
        "    pyplot.imshow(data)\n",
        "    pyplot.draw()\n",
        "    pyplot.title(f'WORLD: {world} EPOCH: {epoch}')\n",
        "    pyplot.ylim(-1, 41)\n",
        "    pyplot.xlim(-1,41)\n",
        "    for z in obstacles:\n",
        "        pyplot.plot(z[0], z[1], marker=\"s\", color = 'k')\n",
        "    for x in good_term_states:\n",
        "        pyplot.plot(x[0], x[1], marker=\"s\", color = 'g')\n",
        "    for y in bad_term_states:\n",
        "        pyplot.plot(y[0], y[1], marker=\"s\", color = 'r')\n",
        "    pyplot.plot(location[0], location[1], marker=\"*\", color = 'indigo')\n",
        "\n",
        "    pyplot.show(block = False)\n",
        "    pyplot.pause(0.0001)\n",
        "\n",
        "    if not os.path.exists(\"./runs/world_{}/attempt_{}\".format(world,run_num)):\n",
        "        os.makedirs(\"./runs/world_{}/attempt_{}\".format(world,run_num))\n",
        "    pyplot.savefig(\"./runs/world_{}/attempt_{}/epoch_{}.png\".format(world, run_num, epoch))\n",
        "\n",
        "enter_world(1325,0)\n",
        "\n",
        "#Q-learning parameters\n",
        "current_reward=-0.1\n",
        "discount_factor = 0.9\n",
        "learning_rate = 0.9\n",
        "\n",
        "#run through 1000 training episodes\n",
        "for episode in range(1000):\n",
        "  #get the starting location for this episode\n",
        "  x, y = 0,0\n",
        "\n",
        "  visited = [] #\n",
        "  obstacles = []#\n",
        "  bad_term_states = []\n",
        "  good_term_states = []\n",
        "  pyplot.figure(1, figsize=(10,10))\n",
        "  curr_board = [[float('-inf')] * 40 for temp in range(40)]\n",
        "  visited.append(tuple(x,y)) #\n",
        "\n",
        "  learning_rate = learning_rate*discount_factor**(episode//100)\n",
        "  while not is_terminal(current_reward):\n",
        "    # select an action\n",
        "    action = get_action(x, y)\n",
        "\n",
        "    #store the current position\n",
        "    current_x, current_y = x, y\n",
        "    x, y, current_reward = get_pos_and_reward(action,current_x, current_y)\n",
        "\n",
        "    if(current_reward >=0):\n",
        "        good_term_states.append(tuple(current_x,current_y))\n",
        "    else:\n",
        "        bad_term_states.append(tuple(current_x,current_y))\n",
        "\n",
        "    current_q_value = q_values[current_x, current_y, action]\n",
        "    temporal_difference = current_reward + (discount_factor * np.max(q_values[x, y])) - current_q_value\n",
        "\n",
        "    curr_board[current_x][current_y] = 1\n",
        "    for i in range (len(curr_board)):\n",
        "        for j in range(len(curr_board)):\n",
        "            if (curr_board[i][j] != 0):\n",
        "                curr_board[i][j] -= .1\n",
        "    for obstacle in obstacles:\n",
        "        if obstacle in visited:\n",
        "            obstacles.remove(obstacle)\n",
        "    update_grid(curr_board, good_term_states, bad_term_states, obstacles, epoch, worldId, tuple(x,y))\n",
        "\n",
        "\n",
        "    #update the q-value\n",
        "    new_q_value = current_q_value + (learning_rate * temporal_difference)\n",
        "    q_values[current_x, current_y, action] = new_q_value\n",
        "print('Complete learning.')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'code': 'FAIL', 'message': 'Cannot enter the world.  You are currently in world: 0'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d909ce891215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mcurr_board\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mvisited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple expected at most 1 arguments, got 2"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#play in the world with the trained Q-value table\n",
        "\"\"\"def play():\n",
        "    x, y = 0, 0\n",
        "    current_reward=-0.1\n",
        "    path = []\n",
        "    path.append([x, y])\n",
        "    #keep moving until it is terminal\n",
        "    while not is_terminal(current_reward):\n",
        "      #get the best action\n",
        "      action = get_action(x, y)\n",
        "      #move to the new state with the best action\n",
        "      x, y, current_reward = get_pos_and_reward(action,x,y)\n",
        "      #add the new position to path\n",
        "      path.append([x, y])\n",
        "    return path\n",
        "\n",
        "\n",
        "#enter_world(1308,0)\n",
        "#print(play())\n",
        "get_state(1308,\"N\",0,39,39)\"\"\"\n"
      ],
      "metadata": {
        "id": "Et7ktEyTa_Pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bd532194-5f14-4b67-9b3b-d7a8c27c4015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def play():\\n    x, y = 0, 0\\n    current_reward=-0.1\\n    path = []\\n    path.append([x, y])\\n    #keep moving until it is terminal\\n    while not is_terminal(current_reward):\\n      #get the best action\\n      action = get_action(x, y)\\n      #move to the new state with the best action\\n      x, y, current_reward = get_pos_and_reward(action,x,y)\\n      #add the new position to path\\n      path.append([x, y])\\n    return path\\n\\n\\n#enter_world(1308,0)\\n#print(play())\\nget_state(1308,\"N\",0,39,39)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}